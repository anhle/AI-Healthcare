{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Ex_16_translation_performance.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anhle/AI-Healthcare/blob/master/AI_2D/Ex/Ex_16_translation_performance.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4stCLzNz0gT-",
        "colab_type": "text"
      },
      "source": [
        "**Translate Performance into Clinical Utility** \n",
        "\n",
        "The output of the last layer of your CNN will output a probability that an image belongs to a given class. The last step in assessing the performance of your model is choosing the appropriate cut-off threshold for this probability value such that the model behaves in a way that is clinically optimal. Changing this threshold will change the true positive, false positive, false negative, and true negative rates, which we learned about in Lesson 1. This will, in turn, change the precision and recall of our model. Precision and recall are important concepts in clinical testing, and usually, one is optimized at the expense of another.\n",
        "\n",
        "In this exercise, you'll be given a dataframe with ground truth labels as well as probabilities output by an algorithm that you developed to classify images as having a malignant tumor or not. Your job is to generate a Precision-Recall curve as well as an optional ROC AUC curve with the data in this dataframe.\n",
        "\n",
        "Once you create these curves, choose two different thresholds: one that favors precision and one that favors recall. Use these thresholds to calculate two separate F1 scores. Then, also calculate the accuracy of your algorithm using these two different thresholds and think about why accuracy is or is not a good choice of performance statistic for your data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTSiXy2a0di_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import numpy as np       # linear algebra\n",
        "import pandas as pd      # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import binarize\n",
        "\n",
        "from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score, plot_precision_recall_curve, f1_score, confusion_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNUie1Js0djD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "performances = pd.read_csv('https://raw.githubusercontent.com/anhle/AI-Healthcare/master/AI_2D/Ex/data/performances.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VqAS-uJ0djG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "performances.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrYXICME0djI",
        "colab_type": "text"
      },
      "source": [
        "### Exercise Solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3J7JSmC0djJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function to plot the roc_curve. You can utilize roc_curve and auc imported above\n",
        "def plot_roc_curve(t_y, p_y):\n",
        "    fig, c_ax = plt.subplots(1,1, figsize = (9, 9))\n",
        "    fpr, tpr, thresholds = roc_curve(t_y, p_y)\n",
        "    c_ax.plot(fpr, tpr, label = '%s (AUC:%0.2f)'  % ('Pneumonia', auc(fpr, tpr)))\n",
        "    c_ax.legend()\n",
        "    c_ax.set_xlabel('False Positive Rate')\n",
        "    c_ax.set_ylabel('True Positive Rate')\n",
        "    \n",
        "# function to plot the precision_recall_curve. You can utilizat precision_recall_curve imported above\n",
        "def plot_precision_recall_curve(t_y, p_y):\n",
        "    fig, c_ax = plt.subplots(1,1, figsize = (9, 9))\n",
        "    precision, recall, thresholds = precision_recall_curve(t_y, p_y)\n",
        "    c_ax.plot(recall, precision, label = '%s (AP Score:%0.2f)'  % ('Pneumonia', average_precision_score(t_y,p_y)))\n",
        "    c_ax.legend()\n",
        "    c_ax.set_xlabel('Recall')\n",
        "    c_ax.set_ylabel('Precision')\n",
        "\n",
        "# function to calculate the F1 score\n",
        "def  calc_f1(prec,recall):\n",
        "    return 2*(prec*recall)/(prec+recall)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UtIVgwm0djL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_roc_curve(performances['ground_truth'],performances['probability'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VH-1ghCy0djO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_precision_recall_curve(performances['ground_truth'],performances['probability'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVpNDpka0djQ",
        "colab_type": "text"
      },
      "source": [
        "### Calculate F1 Score\n",
        "Calculate F1 score for two different scenarios here, \n",
        "1. Choose a threshold that favors precision, \n",
        "2. Choose a threshold that favors recall\n",
        "\n",
        "Print the precision, recall, threshold, and F1 score for each scenario. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRAsgGTj0djR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "precision, recall, thresholds = precision_recall_curve(performances['ground_truth'],performances['probability'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oWDJoA0o0djU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Look at the threshold where precision is 0.8\n",
        "precision_value = 0.8\n",
        "idx = (np.abs(precision - precision_value)).argmin() \n",
        "print('Precision is: '+ str(precision[idx]))\n",
        "print('Recall is: '+ str(recall[idx]))\n",
        "print('Threshold is: '+ str(thresholds[idx]))\n",
        "print('F1 Score is: ' + str(calc_f1(precision[idx],recall[idx])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKxLKiTD0djX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Look at the threshold where recall is 0.8\n",
        "recall_value = 0.8\n",
        "idx = (np.abs(recall - recall_value)).argmin() \n",
        "print('Precision is: '+ str(precision[idx]))\n",
        "print('Recall is: '+ str(recall[idx]))\n",
        "print('Threshold is: '+ str(thresholds[idx]))\n",
        "print('F1 Score is: ' + str(calc_f1(precision[idx],recall[idx])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-9OmRln0djZ",
        "colab_type": "text"
      },
      "source": [
        "**As we can see, the thresholds are really different for the two cases.**\n",
        "\n",
        "--- \n",
        "\n",
        "### Examine the _accuracy_ under the two scenarios\n",
        "Now look at what the _accuracy_ of our model would be using those two threshold values. **Hint, you can add a new column with value 0 or 1 as predictions based on the threshold you choose, then you can calculate accuracy using ground truth and the prediction you just made.**\n",
        "\n",
        "Print the _accuracy_ in both the cases, and think about why accuracy really isn't a great performance statistic when evaluating and interpreting the utility of our models. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54ZHl03k0dja",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "probs = performances['probability']\n",
        "t1 = (probs > 0.79)\n",
        "t2 = (probs > 0.39)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23jfnxUC0djc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t1 = t1.replace(True,1).replace(False,0)\n",
        "t2 = t2.replace(True,1).replace(False,0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4gC8MnN0djf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "compare_t1 = (t1 == performances['ground_truth'])\n",
        "compare_t2 = (t2 == performances['ground_truth'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JvJ1e2Z0djh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Accuracy at threshold 1: ' + str(len(compare_t1[compare_t1])/len(performances)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNunXJzq0djj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Accuracy at threshold 2: ' + str(len(compare_t2[compare_t2])/len(performances)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vAYiMR3R0djl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
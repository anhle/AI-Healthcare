{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Lesson_EHR_Data_Security_and_Analysis_Screencast.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anhle/AI-Healthcare/blob/master/AI_EHR/Ex/Lesson_EHR_Data_Security_and_Analysis_Screencast.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SBhghIa7-nY",
        "colab_type": "text"
      },
      "source": [
        "**Reasons EDA is important**\n",
        "\n",
        "* EDA can enable you to discover features or data transformations/aggregations that might have data leakage. This can save a tremendous amount of time and prevent you from building a flawed model.\n",
        "\n",
        "* EDA can help you better translate and define modeling objectives and corresponding evaluation metrics from a machine learning/data science and business perspective.\n",
        "\n",
        "* EDA can help inform strategies for handling missing/null/zero valued data. This is a common issue that you will encounter with EHR data that you will have missing values and have to determine imputing strategies accordingly.\n",
        "\n",
        "* EDA can help to identify subsets of features to utilize for feature engineering and modeling along with appropriate feature transformations based off of type (e.g. categorical vs numerical features)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOZMuKQx7lRg",
        "colab_type": "text"
      },
      "source": [
        "## 1. Dataset Schema Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipt70rlC7lRh",
        "colab_type": "text"
      },
      "source": [
        "We will use the following UCI dataset for this lesson and the related exercise.\n",
        "\n",
        "**Dataset**: Heart Disease Dataset donated to UCI ML Dataset Repository https://archive.ics.uci.edu/ml/datasets/heart+Disease. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zz6FNxs87lRi",
        "colab_type": "text"
      },
      "source": [
        "**Modeling Objective:** Predict the incidence of heart disease"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShfnBI187lRl",
        "colab_type": "text"
      },
      "source": [
        "Below is a list of areas that we will be looking for in our exploratory data analysis.\n",
        "- Value Distributions - Is the dataset feature uniform, normal, skewed and severely unbalanced?\n",
        "- Scale of Numerical Features\n",
        "- Missing Values\n",
        "- High Cardinality\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ku2FlPg87lRm",
        "colab_type": "text"
      },
      "source": [
        "**Dataset Schema**: The schema for the dataset that we will be using is on the page https://archive.ics.uci.edu/ml/datasets/heart+Disease  under the **\"Attribute Information\"** header. Please note that only 14 attributes are used and listed below\n",
        "1. age: age in years\n",
        "2. sex: sex (1 = male; 0 = female)\n",
        "3. cp: chest pain type\n",
        "    * Value 1: typical angina\n",
        "    * Value 2: atypical angina\n",
        "    * Value 3: non-anginal pain\n",
        "    * Value 4: asymptomatic\n",
        "4. trestbps: resting blood pressure (in mm Hg on admission to the hospital)\n",
        "5. chol: serum cholestoral in mg/dl\n",
        "6. fbs: (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false)\n",
        "7. restecg: resting electrocardiographic results\n",
        "    * Value 0: normal\n",
        "    * Value 1: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV)\n",
        "    * Value 2: showing probable or definite left ventricular hypertrophy by Estes' criteria\n",
        "8. thalach: maximum heart rate achieved\n",
        "9. exang: exercise induced angina (1 = yes; 0 = no)\n",
        "10. oldpeak: ST depression induced by exercise relative to rest\n",
        "11. slope: the slope of the peak exercise ST segment\n",
        "    * Value 1: upsloping\n",
        "    * Value 2: flat\n",
        "    * Value 3: downsloping\n",
        "12. ca: number of major vessels (0-3) colored by flourosopy\n",
        "13. thal:  3 = normal; 6 = fixed defect; 7 = reversable defect\n",
        "14. num: diagnosis of heart disease (angiographic disease status)\n",
        "    * Value 0: < 50% diameter narrowing\n",
        "    * Value 1: > 50% diameter narrowing\n",
        "    * Values >1: linking to attributes 59 through 68, which are vessels (we won't focus on this for this course)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBGjxLoc7lRm",
        "colab_type": "text"
      },
      "source": [
        "### OPTIONAL- Use Tensorflow Data Validation (TFDV) for EDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JOqrI-T7lRn",
        "colab_type": "text"
      },
      "source": [
        "You are free to use your tool of choice to explore the data and create an EDA report at the end and TFDV currently has some bugs with the latest version of Chrome. The intention of this lesson is to expose you to TFDV as an option to explore your data. While there are other tools for exploratory data analysis, below are some reasons that TFDV can be helpful:\n",
        "* Interactive and simple descriptive statistics visualization tool  \n",
        "* Scales to large datasets\n",
        "    * It uses \"Apache Beam's data-parallel processing framework to scale the computation of statistics over large datasets.\"  \n",
        "* Can be used to detect anomalies and drift with new data or differences between training and testing splits\n",
        "\n",
        "Before building a machine learning model, we must first analyze the dataset and assess for common issues that may require preprocessing. We will use the TFDV library to help analyze and visualize the dataset. Some of the information has been adapted from the TFDV page(https://www.tensorflow.org/tfx/data_validation/get_started. \n",
        "\n",
        "**IMPORTANT** You must use the Chrome browser to see the TFDV library visualizations.\n",
        "\n",
        "NOTE: Please note that there are other ways we can explore and analyze the data but we will focus on these areas for the course.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6QSbPGy7lRn",
        "colab_type": "text"
      },
      "source": [
        "### ETL "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPJMyys87lRo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sklearn\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWuYKlgw7lRr",
        "colab_type": "text"
      },
      "source": [
        "**NOTE:** For this lesson and exercise we will use the processed not the raw dataset provided, so the categorical feature values have already been converted to numerical values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57XBVQqq7lRs",
        "colab_type": "text"
      },
      "source": [
        "For this exercise we will use the processed Cleveland Clinic dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVDihgVW7lRt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#https://archive.ics.uci.edu/ml/datasets/heart+Disease, Cleveland dataset\n",
        "processed_cleveland_path = \"https://raw.githubusercontent.com/anhle/AI-Healthcare/master/AI_EHR/Ex/data/processed.cleveland.txt\"\n",
        "column_header_list = [\n",
        "    'age',\n",
        "   'sex',\n",
        "   'cp',\n",
        "   'trestbps',\n",
        "     'chol',\n",
        "      'fbs',\n",
        "      'restecg',\n",
        "      'thalach',\n",
        "       'exang',\n",
        "      'oldpeak',\n",
        "       'slope',\n",
        "       'ca',\n",
        "        'thal', \n",
        "     'num_label'\n",
        "]\n",
        "processed_cleveland_df = pd.read_csv(processed_cleveland_path, names=column_header_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VeRwtonU7lRv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "processed_cleveland_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXpv9gc77lRz",
        "colab_type": "text"
      },
      "source": [
        "## 2. Analyze Value Distributions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PZcm-z-_tiv",
        "colab_type": "text"
      },
      "source": [
        "* Normal is the well-known bell-curve that most people are familiar with and is also referred to as a Gaussian distribution\n",
        "* The uniform distribution is where the unique values have almost the same frequency and this is important b/c this might indicate some issue with the data\n",
        "* Skewed/unbalanced data distributions as the name indicates are where a smaller subset of values or a single value dominates\n",
        "* Bimodal and Poisson but for the scope of this course we will not cover those"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkJR3w-q7lRz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# visualize categorical distributions\n",
        "def visualize_distributions(df, c):\n",
        "    df[c].value_counts().plot(kind='bar')\n",
        "    plt.show()\n",
        "    plt.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8pELCgk7lR2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "example_column1 = \"sex\"\n",
        "print(\"Distribution for {} feature\".format(example_column1))\n",
        "processed_cleveland_df[example_column1].replace({1:\"male\", 0:\"female\"}).value_counts().plot(kind='bar')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjOJlJEb7lR5",
        "colab_type": "text"
      },
      "source": [
        "Next, we will look at another categorical feature chest pain."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfo7OSL77lR6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "example_column2 = \"cp\"\n",
        "print(\"Distribution for {} feature\".format(example_column2))\n",
        "processed_cleveland_df[example_column2].replace({1: \"typical angina\",\n",
        "2: \"atypical angina\",\n",
        "3: \"non-anginal pain\",\n",
        "4: \"asymptomatic\" }).value_counts().plot(kind='barh')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tiLZlmGB7lR9",
        "colab_type": "text"
      },
      "source": [
        "### Review of normal and uniform distributions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJlZHZ7z7lR-",
        "colab_type": "text"
      },
      "source": [
        "**Normal Distribution**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8l9vRxh7lR-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mu, sigma = 100, 17.0 # mean and standard deviation\n",
        "norm_dist = np.random.normal(mu, sigma, 100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYLvF3dI7lSB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "norm_ax = sns.distplot(norm_dist, kde=False )\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NI3yf_Kt7lSE",
        "colab_type": "text"
      },
      "source": [
        "**Uniform Distribution**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-pkm7qy7lSE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "uniform_dist = np.random.uniform(-1,0,1000)\n",
        "uniform_ax = sns.distplot(uniform_dist, kde=False )\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omIG_o197lSH",
        "colab_type": "text"
      },
      "source": [
        "**What type of distribution is this?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzaD2lon7lSH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# numerical field histogram\n",
        "processed_cleveland_df['trestbps'].hist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQbt2JP97lSK",
        "colab_type": "text"
      },
      "source": [
        "## 3. Missing Values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQaCvvGT_V0d",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Missing values are especially common in healthcare where you may have incomplete records or some fields are sparsely populated\n",
        "\n",
        "Missing Data Classification:\n",
        "\n",
        "* MCAR which stands for Missing Completely at Random. This means that the data is missing due to something unrelated to the data and there is no systematic reason for the missing data. In other words, there is an equal probability that data is missing for all cases. This is often due to some instrumentation like a broken instrument or process issue where some of the data is randomly missing.\n",
        "\n",
        "* MAR refers to Missing at Random and this is the opposite case where there is some systematic relationship between data and the probability of missing data. For example, there might be some missing demographics choices in surveys.\n",
        "\n",
        "* MNAR is a Missing Not at Random and this usually means there is a relationship between a value in the dataset and the missing values.\n",
        "\n",
        "Understanding why data is missing help with choosing the best imputing method to fill or drop the values in your dataset.\n",
        "\n",
        "Code Concepts\n",
        "Create a function to check the percent of missing and zero values you have."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k54V8JPf7lSK",
        "colab_type": "text"
      },
      "source": [
        "### Scaling of numerical features \n",
        "- Compare min and max and see if scale is large "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCzQJRVM7lSK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "numerical_feature_list = ['age',  'trestbps', 'chol', 'thalach', 'oldpeak' ]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAu1JyR17lSN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "processed_cleveland_df[numerical_feature_list].describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lhNXMpF7lSQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Missing values\n",
        "def check_null_values(df):\n",
        "    null_df = pd.DataFrame({'columns': df.columns, \n",
        "                            'percent_null': df.isnull().sum() * 100 / len(df), \n",
        "                           'percent_zero': df.isin([0]).sum() * 100 / len(df)\n",
        "                           } )\n",
        "    return null_df "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MzQ7EEm7lST",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "null_df = check_null_values(processed_cleveland_df)\n",
        "null_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DyrvSyfKBzVd",
        "colab_type": "text"
      },
      "source": [
        "View the results and see if there are any values that stand out. Again you may need to deal with different columns in different ways depending on their type and reason for missing or zero values.\n",
        "\n",
        "Additional Resources:\n",
        "* [Imputation Methods](https://towardsdatascience.com/6-different-ways-to-compensate-for-missing-values-data-imputation-with-examples-6022d9ca0779)\n",
        "* [Advanced Imputation Methods](https://www.sciencedirect.com/science/article/pii/S2352914819302783)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0k4xaqSY7lSV",
        "colab_type": "text"
      },
      "source": [
        "## 4. Outliers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cyrok-rf7lSW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.boxplot(y=processed_cleveland_df['age'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNx2I02_7lSY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.boxplot(y=processed_cleveland_df['chol'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugxAGhKu7lSb",
        "colab_type": "text"
      },
      "source": [
        "## 5. High Cardinality"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYtD2aYD7lSb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "def create_cardinality_feature(df):\n",
        "    num_rows = len(df)\n",
        "    random_code_list = np.arange(100, 1000, 1)\n",
        "    return np.random.choice(random_code_list, num_rows)\n",
        "    \n",
        "def count_unique_values(df, cat_col_list):\n",
        "    cat_df = df[cat_col_list]\n",
        "    cat_df['principal_diagnosis_code'] = create_cardinality_feature(cat_df)\n",
        "    #add feature with high cardinality\n",
        "    val_df = pd.DataFrame({'columns': cat_df.columns, \n",
        "                       'cardinality': cat_df.nunique() } )\n",
        "    return val_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RqvbGd6o7lSd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "categorical_feature_list = [ 'sex', 'cp', 'restecg', 'exang', 'slope', 'ca', 'thal']\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8Yfl0nn7lSf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_df = count_unique_values(processed_cleveland_df, categorical_feature_list) \n",
        "val_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4-7Eywp7lSi",
        "colab_type": "text"
      },
      "source": [
        "# 6. Demographic Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aoqoLIWH7lSj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#convert age to bins\n",
        "demo_features = ['sex',  'age' ]\n",
        "demo_df = processed_cleveland_df[demo_features].replace({1:\"male\", 0:\"female\"})\n",
        "age_bins = np.arange(0, 90, 10)\n",
        "a_bin = [str(x) for x in np.arange(0, 90, 10) ]\n",
        "age_labels = [\"\".join(x) for x in zip( [x + \" - \" for x in a_bin[:-1]], a_bin[1:])]\n",
        "demo_df['age_bins'] = pd.cut(demo_df['age'], bins=age_bins, labels=age_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKGPdsMq7lSl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "demo_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Horf9PB-7lSo",
        "colab_type": "text"
      },
      "source": [
        "### Group by Age Bins"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HqWuyBkC7lSo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ax = sns.countplot(x=\"age_bins\", data=demo_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEByi_c47lSq",
        "colab_type": "text"
      },
      "source": [
        "### Group by Gender and Age Bins"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykIN6dHt7lSr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ax = sns.countplot(x=\"age_bins\", hue=\"sex\", data=demo_df)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}